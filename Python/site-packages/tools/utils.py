#!/usr/bin/env python
# -*- coding: utf-8 -*-

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Some common useful functions
#
#
# Authors: Hao BAI
# Date: 17/10/2018
#
# Version:
#   - 0.0: Initial version
#   
# Comments:
# 
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



#!------------------------------------------------------------------------------
#!                                          MODULES PRÃ‰REQUIS
#!------------------------------------------------------------------------------
#*============================= Modules Personnels =============================

#* ============================== Modules Communs =============================
# import csv, copy
import os
# import numpy
import math
import json
import pandas
import time
import pickle
import pathlib
import datetime
import collections
import subprocess
import itertools
# from scipy import signal
from contextlib import contextmanager
from pathlib import Path
import numpy as np
import multiprocessing
import multiprocessing.pool
from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor



#!------------------------------------------------------------------------------
#!                                         CLASS
#!------------------------------------------------------------------------------
class Input(object):
    """ Save data reading from files
        Attributes:
            data: a dictionary of all data labeled by channel name in FAST [ordered dict]
        Methods:
            get(): get values by channel name or channel number
    """
    def __init__(self, keys=None):
        if keys is None:
            self.data = collections.OrderedDict()
        else:
            self.data = collections.OrderedDict.fromkeys(keys)
    
    def get(self, index):
        if isinstance(index, str):
            return self.data[index] # get by channel name in FAST output file
        elif isinstance(index, int):
            return list(self.data.values())[index-1] # get by channel number
        else:
            raise TypeError("[Error] index type is not allowable, should be str"
                            " or int")

    def getkeys(self):
        return list(self.data.keys())



#!------------------------------------------------------------------------------
#!                                      FUNCTIONS
#!------------------------------------------------------------------------------
@contextmanager
def cd(newdir):
    if isinstance(newdir, Path): newdir = str(newdir)
    prevdir = os.getcwd() # save current working path
    os.chdir(os.path.expanduser(newdir)) # change directory
    try:
        yield
    finally:
        os.chdir(prevdir) # revert to the origin workinng path

def rm(filename):
    ''' Delete a file or symbolic link'''
    if not isinstance(filename, Path):
        filename = Path(filename).absolute()
    else:
        filename = filename.absolute()
    filename.unlink()

def frange(start, stop=None, step=None, precision=None):
    """ Return a list of float numbers
        Syntax:
            range(stop)
            range(start, stop[, step][, precision])
    """
    # check how many inputs
    if stop is None and step is None and precision is None:
        return frange(0.0, start)
    # set default step
    if step is None:
        temp = str(stop).lstrip('1234567890')
        if temp == '':
            step = 1.0
        else:
            step = 10**(-(len(temp)-1))
    # set default precision
    if precision is None:
        fois = 1e7
    else:
        fois = 10**precision
    # amplify float number to int number
    new_start = int(start * fois)
    new_stop = int(stop * fois)
    new_step = int(step * fois)
    r = range(new_start, new_stop, new_step)
    # convert int number to float number
    l = [i/fois for i in r]
    return l

def readfile(filename, delimiter='\t', header=7, datarow=9, unitrow=None,
         endrow=None, ncols=None, format='num', echo=True):
    """ Read a csv-like file into ordered dictionary (by using Python official
        packages)
        Syntax:
            readcsv(filename[, delimiter[, header[, datarow[, echo]]]])
        Input:
            filename: name of file to be opened [string/Path]
            delimiter: seperator [string]
            header: line number that contains column's name [int]
            datarow: line number that contains the first row of data [int]
            unitrow: line number that contains the unit [int]
            endrow: line number that contains the last row of data [int]
            ncols: number of columns that will be loaded [int]
            echo: True/False to print elapsed time on screen [boolean]
    """
    TIK = time.time()
    # check and convert filename type
    if isinstance(filename, Path): filename = str(filename.expanduser())
    # convert number of line to 0-indexed
    header = header - 1
    datarow = datarow - 1
    # preload data
    with open(filename, encoding='ISO-8859-1') as f:
        data = f.readlines()
        length = len(data)
        # set up selector
        selector = list(itertools.repeat(0, length))
        selector[header] = 1
        if unitrow is not None:
            selector[unitrow-1] = 1
        if endrow is not None:
            selector[datarow:endrow] = list(itertools.repeat(1, endrow-datarow))
        else:
            selector[datarow:] = list(itertools.repeat(1, length-datarow))
        data = list(itertools.compress(data, selector))
    # initiate Input instance
    keys = [name.strip() for name in data[0].split(delimiter)]
    result = Input(keys)
    names = data.pop(0).split(delimiter)
    if unitrow is not None:
        units = [unit.strip() for unit in data.pop(0).split(delimiter)]
        for (ind,val) in enumerate(keys):
            result.data[val] = {"Title":names[ind], "Records":[], "Unit":units[ind]}
    else:
        for (ind,val) in enumerate(keys):
            result.data[val] = {"Title":names[ind], "Records":[], "Unit":"(-)"}
    # read numerical data line by line
    if format == "num":
        for line in data:
            for (ind,num) in enumerate(line.split(delimiter)):
                key = keys[ind]
                result.data[key]["Records"].append(float(num))
    elif format == "str":
        for line in data:
            for (ind, num) in enumerate(line.split(delimiter)):
                key = keys[ind]
                try:
                    temp = float(num)
                except:
                    result.data[key]["Records"].append(num)
                else:
                    result.data[key]["Records"].append(temp)
    TOK = time.time()
    if echo:
        print("|-", filename, "loaded by readfile() in", "%.2f" % (TOK-TIK), "s")
    return result

def readcsv(filename, delimiter='\t', header=7, unitrow=None, datarow=9,
            endrow=None, ncols=None, checkNaN=True, echo=True):
    """ Read a csv-like file into ordered dictionary (by using pandas package)
        Syntax:
            readcsv(filename[, delimiter[, header[, datarow[, echo]]]])
        Input:
            filename: name of file to be opened [string/Path]
            delimiter: seperator [string]
            header: line number that contains column's name [int]
            datarow: line number that contains the first row of data [int]
            unitrow: line number that contains the unit [int]
            endrow: line number that contains the last row of data [int]
            ncols: number of columns that will be loaded [int]
            echo: True/False to print elapsed time on screen [boolean]
    """
    TIK = time.time()
    result = Input()
    # check and convert filename type
    if isinstance(filename, Path): filename = str(filename.expanduser())
    # convert number of line to 0-indexed
    header = header - 1
    datarow = datarow - 1
    # check unitrow and initiate line numbers that will be ignored
    if unitrow is not None:
        unitrow = unitrow - 1
        skiprows = range(unitrow+1, datarow)
    else:
        skiprows = range(header+1, datarow)
    # read file by using padas
    #* don't set low_memory=False or dtype=str => will be slower
    if endrow is None:
        dataframe = pandas.read_csv(filename, delimiter=delimiter,header=header,
                                    encoding='ISO-8859-1', skiprows=skiprows,
                                    skip_blank_lines=False)
    else:
        if unitrow is not None:
            nrows = endrow - datarow + 1
        else:
            nrows = endrow - datarow
        dataframe = pandas.read_csv(filename, delimiter=delimiter,header=header,
                                    encoding='ISO-8859-1', skiprows=skiprows,
                                    skip_blank_lines=False, nrows=nrows)
    # read data column by column
    if ncols is not None: 
        size = ncols
    else:
        size = dataframe.columns.size
    for i in range(size):
        name = dataframe.columns[i] # get the column head
        if unitrow is not None: # get unit ONLY if it's a FAST output file (.out)
            unit = dataframe.iloc[0,i].strip() # remove empty space
            records = dataframe.iloc[1:,i].values.astype('float') # get values in float
        else:
            unit = "(-)"
            records = dataframe.iloc[:,i].values.astype('float') # get values in float
        # check NaN values
        if checkNaN:
            if math.isnan(sum(records)):
                for (key,value) in enumerate(records):
                    if math.isnan(value):
                        print("[ALERT] The value of {} on position #{} column #"
                              "{} is NaN or empty !".format(value, key, i))
                raise Warning("[ALERT] NaN value is found in file {}".format(
                              filename, i, name))
        result.data[name.strip()] = {'Title':name, 'Records':list(records),
                                     'Unit': unit}
    TOK = time.time()
    if echo:
        print("|-", filename, "loaded by readcsv() in", "%.2f"%(TOK-TIK), "s")
    return result

#! ERROR: can't run
def readcsv_multiprocess1(list_filename, delimiter='\t', header=7, datarow=9, 
                          unitrow=None, endrow=None, ncols=None, echo=True):
    import sys
    def printer(temp):
        print("OK")

    iterable = [(f, delimiter, header, datarow, unitrow, endrow, ncols, echo)
                for f in list_filename]
    pool = multiprocessing.Pool(7)
    # result = [pool.apply_async(readcsv, args=(f, delimiter, header, datarow, unitrow, endrow, ncols, echo), callback=printer, error_callback=handle_error) for f in list_filename]
    result = pool.starmap_async(
        readcsv, iterable, callback=printer, error_callback=handle_error)
    pool.close()
    pool.join()
    return result

#! WARNING: cost too much memory
def readcsv_multiprocess2(list_filename, delimiter='\t', header=7, datarow=9,
                          unitrow=None, endrow=None, ncols=None, echo=True):
    iterable = [(f, delimiter, header, datarow, unitrow, endrow, ncols, echo)
                for f in list_filename]
    with ProcessPoolExecutor(max_workers=7) as executor:
        futures = []
        for f in list_filename:
            futures.append(executor.submit(readcsv, f, delimiter, header, datarow, unitrow, endrow, ncols, echo))
        for future in as_completed(futures):
            # num = futures.index(future)
            try:
                result = future.result()
            except Exception as e:
                print('raise an exception: {}'.format(e))
            else:
                print('readcsv() = {}'.format(result))
    return futures

def readfwf(filename, widths=(11, 11), header=12, unitrow=None, datarow=14,
            endrow=None, ncols=None, checkNaN=True, echo=True):
    """ Read a fixed-width formatted file into ordered dictionary
        Syntax:
            readfwf(filename[, widths[, header[, datarow[, echo]]]])
        Input:
            filename: name of file to be opened [string]
            widths: list of field widths [list of int]
            header: line number that contains column's name [int]
            datarow: line number that contains the first row of data [int]
            echo: True/False to print elapsed time on screen [boolean]
    """
    TIK = time.time()
    result = Input()
    # convert number of line to 0-indexed
    header = header - 1
    datarow = datarow - 1
    # check unitrow and initiate line numbers that will be ignored
    if unitrow is not None:
        unitrow = unitrow - 1
        skiprows = range(unitrow+1, datarow)
    else:
        skiprows = range(header+1, datarow)
    if endrow is None:
        dataframe = pandas.read_fwf(filename, widths=widths, header=header,
                                    encoding='ISO-8859-1', skiprows=skiprows,
                                    skip_blank_lines=False)
    else:
        if unitrow is not None:
            nrows = endrow - datarow + 1
        else:
            nrows = endrow - datarow
        dataframe = pandas.read_fwf(filename, widths=widths, header=header,
                                    encoding='ISO-8859-1', skiprows=skiprows,
                                    skip_blank_lines=False, nrows=nrows)
    # dataframe = pandas.read_fwf(filename, widths=widths, header=header)
    # skip rows between header and datarow
    # if datarow-header >= 2:
    #     endIndex = (datarow-header)-1
    #     dataframe.drop(list(range(endIndex)), inplace=True)
    # read data column by column
    if ncols is not None:
        size = ncols
    else:
        size = dataframe.columns.size
    for i in range(size):
        name = dataframe.columns[i] # get the column head
        if unitrow is not None:
            unit = dataframe.iloc[0,i].strip()  # remove empty space
            records = dataframe.iloc[1:,i].values.astype('float')
        else:
            unit = "(-)"
            records = dataframe.iloc[:, i].values.astype('float')
        # check NaN values
        if checkNaN:
            if math.isnan(sum(records)):
                for (key, value) in enumerate(records):
                    if math.isnan(value):
                        print("[ALERT] The value of {} on position #{} column #"
                              "{} is NaN or empty !".format(value, key, i))
                raise Warning("[ALERT] NaN value is found in file {}".format(
                              filename, i, name))
        result.data[name.strip()] = {'Title':name, 'Records':list(records),
                                     'Unit': unit}
    TOK = time.time()
    if echo:
        print("|-", filename, "loaded by readcsv() in", "%.2f"%(TOK-TIK), "s")
    return result


def save_json(data, filename, replace):
    # convert unserializable objects
    if isinstance(data, dict):
        for key, value in data.items():
            if isinstance(value, np.ndarray):
                data[key] = value.tolist()
            elif isinstance(value, list):
                for i, v in enumerate(value):
                    if isinstance(v, np.ndarray):
                        value[i] = v.tolist()
            elif isinstance(value, dict):
                for k, v in value.items():
                    if isinstance(v, np.ndarray):
                        value[k] = v.tolist()
    # write data by using json format
    encode = json.dumps(data, indent=4)
    # check if the file already exists
    p = pathlib.Path(filename)
    if p.exists() and not replace:
        oldname, extension = p.stem, p.suffix
        suffix = str(datetime.datetime.now())
        p = p.with_name(oldname+"_"+suffix+extension)
    # write to file
    with p.open("w", encoding="utf-8") as f:
        f.write(encode)


def load_json(filename):
    p = pathlib.Path(filename)
    with p.open("r", encoding="utf-8") as f:
        return json.load(f)


def save_binary(data, filename, replace):
    # check if the file already exists
    p = pathlib.Path(filename)
    if p.exists() and not replace:
        oldname, extension = p.stem, p.suffix
        suffix = str(datetime.datetime.now())
        p = p.with_name(oldname+"_"+suffix+extension)
    # write to file
    with p.open("wb") as f:
        pickle.dump(data, f)


def load_binary(filename):
    p = pathlib.Path(filename)
    with p.open("r") as f:
        return pickle.load(f)


def timer(function):
    """ Decorator for timing
    """
    def wrapper(*args, **kwargs):
        tik = time.time()
        result = function(*args, **kwargs)
        tok = time.time()
        print("[INFO] {}() finished in {:.3f} s".format(function.__name__, 
            tok-tik))
        return result
    return wrapper

def handle_error(error):
    """ What to do if an error occurs in multiprocessing
    """
    with open('error.log','a') as f:
        f.write('{}\n'.format(error))
    print("[ERROR] An error occurs during multiprocessing:\n        {}".format(error))

def format_seconds(timeInSeconds):
    """ Format time in seconds
    """
    day = 24*60*60
    hour = 60*60
    minute = 60
    if timeInSeconds > day:
        days = timeInSeconds//day
        rest = format_seconds(timeInSeconds%day)
        return '{:.0f} days {}'.format(days, rest)
    elif timeInSeconds > hour:
        hours = timeInSeconds//hour
        rest = format_seconds(timeInSeconds%hour)
        return '{:.0f} hours {}'.format(hours, rest)
    elif timeInSeconds > minute:
        minutes = timeInSeconds//minute
        rest = format_seconds(timeInSeconds%minute)
        return '{:.0f} minutes {}'.format(minutes, rest)
    else:
        return '{:.2f} seconds'.format(timeInSeconds)


def compress(filebase, source, remove_source, level=-9, background=True):
    if remove_source == True:
        command = 'GZIP={2} tar -czf {0}.tgz {1} && rm {1}'.format(
            filebase, source, level)
    else:
        command = 'GZIP={2} tar -czf {0}.tgz {1}'.format(
            filebase, source, level)
    if background == True:
        command = command + ' &'
    try:
        output = subprocess.check_call([command], shell=True)
    except Exception as e:
        raise e
    return filebase


def add_failed_seeds(programName, identity):
    """ Collect simulation identities (seed ID, filebase name, ...) that has an error during execution
    """
    # prepare collected file
    filename = Path('~/Eolien/Parameters/NREL_5MW_Onshore/Wind')
    filename = filename.joinpath('failedRuns{}.json'.format(programName)).expanduser()
    # initiate data
    if filename.exists():
        data = json.loads(filename.read_text())
    else:
        data = []
    # convert identity to seed if it's in string format
    if isinstance(identity, str):
        seed = identity.split('_')
        seed[1] = seed[1].rstrip('mps')
        identity = seed
    data.append(identity)
    # write to disk
    encode = json.dumps(data, indent=4)
    filename.write_text(encode)


def find(path, pattern, size=None):
    """ size: minimum size in bytes (1 GB = 1024 MB = 1024^2 KB = 1024^3 Bytes) [num]
    """
    with cd(path):
        p = Path().expanduser()
        matched = sorted(p.glob(pattern))
        if size is None:
            result = [x.stem for x in matched]
        else:
            result = [x.stem for x in matched if x.stat().st_size >= size]
    return result


def normalize(x, data_range=None):
    ''' Normalize x to a specified range
    '''
    X = np.array(x)
    if data_range is None:
        data_range = (X.min(axis=0), X.max(axis=0))
    return (X - data_range[0]) / (data_range[1] - data_range[0])


#TODO
def multiprocess(function):
    """ Decorator for multiprocessing
    """
    def wrapper(self, list_for_map):
        pool = multiprocessing.Pool()
        # pool = multiprocessing.pool.ThreadPool()
        # [pool.apply_async(function, args=(variables,)) for filebase in list_filebase]
        result = pool.map(function, list_for_map)
        pool.close()
        pool.join()
        return result
    return wrapper


#!------------------------------------------------------------------------------
#!                                     MAIN FUNCTION
#!------------------------------------------------------------------------------
@timer
def main():
    # print("frange(3) =", frange(3))
    # print("frange(3,5) =", frange(3,5))
    # print("frange(3,4.02) =", frange(3,4.02))
    # print("frange(5,9,2) =", frange(5,9,2))
    # print("frange(5,7,0.5) =", frange(5,7,0.5))
    # print("-----")

    # test = readfile('csv.out')

    # test = readcsv('csv.out', unitrow=8)
    # print(test.get(2))
    # print(test.get('Wind1VelX'))
    # test = readfwf('fwf.ext')
    # print(test.get(1))
    # print(test.get('Time'))
    # print(test.getkeys()[0])
    # test.get(1.0) # raise error
    # print("-----")

    # test = readcsv("./readSpeedTest.out", header=7, datarow=6009, endrow=9009)
    # print(test.get(1)["Records"][:3], end="")
    # print(test.get(1)["Unit"], end="")
    # print(test.get(1)["Records"][-3:])
    # test = readcsv("./readSpeedTest.out", unitrow=8, header=7, datarow=6009,
    #                endrow=9009)
    # print(test.get(1)["Records"][:3], end="")
    # print(test.get(1)["Unit"], end="")
    # print(test.get(1)["Records"][-3:])
    # print("-----")

    # test = readfile("./readSpeedTest.out", header=7, unitrow=8, datarow=6009, 
    #                 endrow=9009)
    # print(test.get(1)["Records"][:3], end="")
    # print(test.get(1)["Unit"], end="")
    # print(test.get(1)["Records"][-3:])
    # print("-----")

    datafolder = Path("~/Eolien/Parameters/NREL_5MW_Onshore/Output/DLC2.3"
                      "/reliability").expanduser()
    list_filebase = find(datafolder, "*.out")
    list_filename = [f+".out" for f in list_filebase[:50]]
    with cd(datafolder):
        # method 1
        for f in list_filename:
            # readcsv(f)
            readfile(f)
    #     # method 2
    #     # alldata = readcsv_multiprocess1(list_filename)
    #     # method 3
    #     alldata = readcsv_multiprocess2(list_filename)
    # print("-----")

    # compress('csv.out')
    

    pass


#!------------------------------------------------------------------------------
#!                                         EXÃ‰CUTION
#!------------------------------------------------------------------------------
if __name__ == '__main__':
    main()
